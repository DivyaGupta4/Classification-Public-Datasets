{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code I have tried to implement Logistic Regression using a shallow neural network to demonstrate forward and backward propagation. This is an open source UCI dataset for classifying emails as spam or not spam. This is just for understanding purposes about how gradients are calculated in a neural network and how we can use sigmoid activation function to compute paramters and minimize costs.\n",
    "\n",
    "This also demostrates use of matrix multiplication for faster computation in a neural network using NumPy instead of using for loops that are about 500 times slower.\n",
    "\n",
    "I have tried to implement my learning from DeepLearning Specialization course on Coursera on an open sourse data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression can be easily implemented using Scikit-Learn's functions but this one is just for learning purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset into pandas and convert it to NumPy arrays for computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', header  = None)\n",
    "X = spam.loc[:,0:56]\n",
    "y = spam.loc[:,57]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify = y, random_state = 123)\n",
    "\n",
    "# Converting to NumPy\n",
    "X_train = X_train.as_matrix()\n",
    "y_train = y_train.as_matrix()\n",
    "X_test = X_test.as_matrix()\n",
    "y_test = y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3680\n",
      "Number of testing samples: 921\n",
      "Number of variables: 57\n"
     ]
    }
   ],
   "source": [
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "num_var = X_train.shape[1]\n",
    "train_vector = X_train.T\n",
    "test_vector = X_test.T\n",
    "\n",
    "print (\"Number of training samples: \" + str(num_train))\n",
    "print (\"Number of testing samples: \" + str(num_test))\n",
    "print (\"Number of variables: \" + str(num_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vectors(dim):\n",
    "    w = np.zeros(dim).reshape(dim,1)\n",
    "    b = 0    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1/(1+np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagation(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X) + b)\n",
    "    dw = (1/m)*np.dot(X,(A-Y).T)\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "    \n",
    "    cost = (-1/m)*np.sum(np.log(A)*Y+np.log(1-A)*(1-Y))    \n",
    "    cost = np.squeeze(cost)\n",
    "    gradient = {\"dw\": dw,\n",
    "                \"db\": db}\n",
    "    return gradient, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter(w, b, X, Y, iterations, learning_rate):    \n",
    "    costs = []\n",
    "    grads = {}\n",
    "    for i in range(iterations):\n",
    "        gradient, cost = propagation(w, b, X, Y)\n",
    "        dw = gradient[\"dw\"]\n",
    "        db = gradient[\"db\"]\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)    \n",
    "    parameters = {\"w\": w,\n",
    "                  \"b\": b}\n",
    "    \n",
    "    gradients = {\"dw\": dw,\n",
    "                 \"db\": db}\n",
    "    \n",
    "    return parameters, gradients, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    A = sigmoid(np.dot(w.T,X) + b)\n",
    "    \n",
    "    # The cut-off here is 0.5; This can be changed as per business requirements\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0,i] <= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "        pass    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(X_train, y_train, X_test, y_test, iterations, learning_rate):\n",
    "\n",
    "    w, b = initialize_vectors(X_train.shape[0])\n",
    "    parameters, grads, costs = parameter(w, b, X_train, y_train, iterations, learning_rate)\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    Y_prediction_test = predictions(w, b, X_test)\n",
    "    Y_prediction_train = predictions(w, b, X_train)\n",
    "    \n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - y_test)) * 100))\n",
    "    results = {\"costs\": costs,\n",
    "               \"Y_prediction_test\": Y_prediction_test, \n",
    "               \"Y_prediction_train\" : Y_prediction_train, \n",
    "               \"w\" : w, \n",
    "               \"b\" : b,\n",
    "               \"learning_rate\" : learning_rate,\n",
    "               \"iterations\": iterations}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Logisitc Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 81.71195652173913 %\n",
      "test accuracy: 84.25624321389793 %\n"
     ]
    }
   ],
   "source": [
    "results = log_reg(train_vector, y_train, test_vector, y_test, iterations = 5000, learning_rate = 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty decent accuracy. However, test accuracy is more than train accuracy, which shows underfitting. The code however demonstrates calculation of gradients, parameters and costs like a neural network would compute.\n",
    "I will also upload a notebook showing Logisitic Regression using Scikit-Learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
